{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tensor-header"
   },
   "source": [
    "# üßÆ Advanced AI Tensor Core - Deep Dive\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MASSIVEMAGNETICS/Victor_Synthetic_Super_Intelligence/blob/main/notebooks/04_Advanced_AI_Tensor_Core.ipynb)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Victor's **Tensor Core** provides a complete automatic differentiation engine with:\n",
    "\n",
    "- üéØ **Autograd** - Full backpropagation support\n",
    "- üî¢ **Operations** - Matrix multiplication, element-wise ops\n",
    "- üìà **Activations** - GELU, SiLU, tanh, sigmoid, softplus\n",
    "- ‚öôÔ∏è **Optimizers** - SGD, Adam\n",
    "- üåä **ODE Integration** - Euler, RK4 for continuous dynamics\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Basic tensor operations and autograd\n",
    "2. Building neural network layers\n",
    "3. Training with optimizers\n",
    "4. ODE integration for continuous systems\n",
    "5. Integration with quantum-fractal mesh\n",
    "\n",
    "Let's begin! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "!git clone https://github.com/MASSIVEMAGNETICS/Victor_Synthetic_Super_Intelligence.git\n",
    "%cd Victor_Synthetic_Super_Intelligence\n",
    "!pip install -q numpy>=1.21.0\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/content/Victor_Synthetic_Super_Intelligence')\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "basic-ops"
   },
   "source": [
    "## üî¢ Basic Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tensor-ops"
   },
   "outputs": [],
   "source": [
    "from advanced_ai.tensor_core import Tensor\n",
    "import numpy as np\n",
    "\n",
    "# Create tensors\n",
    "x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "y = Tensor([[5.0, 6.0], [7.0, 8.0]], requires_grad=True)\n",
    "\n",
    "print(\"üìä Input Tensors:\")\n",
    "print(\"x =\", x.data)\n",
    "print(\"y =\", y.data)\n",
    "\n",
    "# Operations\n",
    "print(\"\\nüîß Operations:\")\n",
    "print(\"x + y =\", (x + y).data)\n",
    "print(\"x * y =\", (x * y).data)\n",
    "print(\"x @ y.T =\", x.matmul(y.T()).data)\n",
    "print(\"x.mean() =\", x.mean().data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "autograd"
   },
   "source": [
    "## üéØ Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "backprop"
   },
   "outputs": [],
   "source": [
    "# Create computational graph\n",
    "x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "w = Tensor([[0.5], [0.5]], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "y = x.matmul(w)  # Linear layer\n",
    "z = y.gelu()      # GELU activation\n",
    "loss = z.mean()   # Loss function\n",
    "\n",
    "print(\"Forward Pass:\")\n",
    "print(f\"  Input x: {x.data.shape}\")\n",
    "print(f\"  Weights w: {w.data.shape}\")\n",
    "print(f\"  Output y: {y.data}\")\n",
    "print(f\"  After GELU: {z.data}\")\n",
    "print(f\"  Loss: {loss.data}\")\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\nGradients:\")\n",
    "print(f\"  ‚àáx:\\n{x.grad}\")\n",
    "print(f\"  ‚àáw:\\n{w.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "activations"
   },
   "source": [
    "## üìà Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-activations"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test range\n",
    "x_range = np.linspace(-3, 3, 100)\n",
    "x_tensor = Tensor(x_range.reshape(-1, 1))\n",
    "\n",
    "# Apply activations\n",
    "activations = {\n",
    "    'GELU': x_tensor.gelu(),\n",
    "    'SiLU': x_tensor.silu(),\n",
    "    'Tanh': x_tensor.tanh(),\n",
    "    'Sigmoid': x_tensor.sigmoid()\n",
    "}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i, (name, output) in enumerate(activations.items(), 1):\n",
    "    plt.subplot(1, 4, i)\n",
    "    plt.plot(x_range, output.data.flatten(), linewidth=2)\n",
    "    plt.title(name)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel(f'{name}(x)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## ‚öôÔ∏è Training with Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "optimizer-demo"
   },
   "outputs": [],
   "source": [
    "from advanced_ai.tensor_core import Adam\n",
    "\n",
    "# Create simple dataset (XOR problem)\n",
    "X = Tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "y_true = Tensor([[0.0], [1.0], [1.0], [0.0]])\n",
    "\n",
    "# Initialize weights\n",
    "w1 = Tensor(np.random.randn(2, 4) * 0.5, requires_grad=True)\n",
    "b1 = Tensor(np.zeros((1, 4)), requires_grad=True)\n",
    "w2 = Tensor(np.random.randn(4, 1) * 0.5, requires_grad=True)\n",
    "b2 = Tensor(np.zeros((1, 1)), requires_grad=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam([w1, b1, w2, b2], lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "print(\"üéì Training Neural Network:\\n\")\n",
    "\n",
    "for epoch in range(200):\n",
    "    # Forward pass\n",
    "    h = (X.matmul(w1) + b1).gelu()\n",
    "    y_pred = h.matmul(w2) + b2\n",
    "    \n",
    "    # MSE loss\n",
    "    diff = y_pred - y_true\n",
    "    loss = (diff * diff).mean()\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.data)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch:3d}: Loss = {loss.data:.6f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete! Final loss: {losses[-1]:.6f}\")\n",
    "\n",
    "# Plot loss curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ode"
   },
   "source": [
    "## üåä ODE Integration\n",
    "\n",
    "Solve continuous differential equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ode-demo"
   },
   "outputs": [],
   "source": [
    "from advanced_ai.tensor_core import ODEIntegrator\n",
    "\n",
    "# Simple harmonic oscillator: dx/dt = v, dv/dt = -x\n",
    "def harmonic_oscillator(state):\n",
    "    x, v = state.data[0], state.data[1]\n",
    "    return Tensor([v, -x])\n",
    "\n",
    "# Initialize\n",
    "state = Tensor([1.0, 0.0])  # x=1, v=0\n",
    "integrator = ODEIntegrator(dt=0.01, method='rk4')\n",
    "\n",
    "# Integrate\n",
    "trajectory = [state.data.copy()]\n",
    "for _ in range(628):  # About 2œÄ seconds\n",
    "    state = integrator.step(state, harmonic_oscillator)\n",
    "    trajectory.append(state.data.copy())\n",
    "\n",
    "trajectory = np.array(trajectory)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(trajectory[:, 0], label='Position (x)', linewidth=2)\n",
    "plt.plot(trajectory[:, 1], label='Velocity (v)', linewidth=2)\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Harmonic Oscillator')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(trajectory[:, 0], trajectory[:, 1], linewidth=2)\n",
    "plt.xlabel('Position (x)')\n",
    "plt.ylabel('Velocity (v)')\n",
    "plt.title('Phase Space')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ ODE integration complete - perfect circular phase space!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary-tensor"
   },
   "source": [
    "## üéâ Summary\n",
    "\n",
    "You learned:\n",
    "\n",
    "‚úÖ **Tensor operations** - Matrix math with autograd  \n",
    "‚úÖ **Backpropagation** - Automatic gradient computation  \n",
    "‚úÖ **Activations** - GELU, SiLU, tanh, sigmoid  \n",
    "‚úÖ **Optimization** - Adam optimizer for training  \n",
    "‚úÖ **ODE integration** - Continuous dynamics with RK4  \n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "- **[NLP Integration](./05_NLP_Integration.ipynb)** - Language processing\n",
    "- **[Genesis Engine](./06_Genesis_Engine.ipynb)** - Full quantum-fractal system\n",
    "- **[Complete System Demo](./07_Complete_System_Demo.ipynb)** - Integration\n",
    "\n",
    "---\n",
    "\n",
    "**Built with üß† by MASSIVEMAGNETICS**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
